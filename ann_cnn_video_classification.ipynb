{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LETS START THE TESORFLOW AFTER THE SKIT-LEARN"
      ],
      "metadata": {
        "id": "hEJYRZh0JoyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "how the tensorflow used in the ann"
      ],
      "metadata": {
        "id": "B8w152SMJy_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jwbmRHJcKItR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA_zSyf1I45e"
      },
      "outputs": [],
      "source": [
        "#to load the file data\n",
        "df=pd.read_csv(\"ata file name\")\n",
        "X=pd.iloc[:.:-1].values\n",
        "Y=pd.lioc[:,-1].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# regression and the classification"
      ],
      "metadata": {
        "id": "keTPh9OnaHG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data prerprocesssing phase in case of the regession and the classification"
      ],
      "metadata": {
        "id": "k1bVwF54Nn2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#after load the data\n",
        "\n",
        "#impute the data\n",
        "from sklearn.impute import SimpleImputer\n",
        "si=SimpleImputer(missing_values=\"np.nan\" ,strategy=\"mean\" )\n",
        "X=si.fit_transform(X[:,2])\n",
        "\n",
        "#encoding the data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.prepreocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.Compose import ColumnTransformer\n",
        "\n",
        "le=LabelEncoder()\n",
        "oe=OneHotencoder()\n",
        "oe = OrdinalEncoder(categories=[['Poor','Average','Good'],['School','UG','PG']])\n",
        "ct=ColumnTransformer()\n",
        "ct=ColumnTransformer(transformers=[\"encoder\" , OneHotEncoder() ,[]])\n",
        "\n",
        "X_train =cf.fit_transform(X_train)\n",
        "X[:,2]=le.fit_transform(X[:,2])\n",
        "-\n",
        "#spliting the data in the training and the test set\n",
        "from slearn.model_selection import train_test_split\n",
        "X_train ,y_train ,X_test , y_test=train_test_split(X ,y, test_size=0.2 , random_state=0)\n",
        "\n",
        "#scaling the data-here we used the scikit-learn library\n",
        "from skearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "ss=StandardScaler()\n",
        "ms=MinMaxScaler()\n",
        "X_train=ss.fit_transform(X_train)\n",
        "X_test=ss.traform(X_test)\n",
        "\n",
        "#after performing the data preprocessing phase\n",
        "model=tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=256 , activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(units=45  ,activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(units = 45 , activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(units = 45 , activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(units =35 , activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(units =35 , activation=\"softmax\"))\n",
        "\n",
        "#next step is compile the model\n",
        "model.compile(optimizer=\"adam\" , loss=\"Catogorical_cross_entropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#train the model\n",
        "model.fit(epochs=10 , learning_rate=0.01 , batch_size=32 , verbose=1)"
      ],
      "metadata": {
        "id": "LqzYyJBwNxW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# image_data"
      ],
      "metadata": {
        "id": "yJ2GPRuiZvRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to load the image data\n",
        "df=tf.keras.preprocessing.image_data_from_directory(\n",
        "    \"data name\",\n",
        "    image_size=(256, 256),\n",
        "    seed=123,\n",
        "    n_classes=3,\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "#see the data shape and the size\n",
        "for image_batch , label_batch in dataset.take(1):\n",
        "    print(image_batch.shape)\n",
        "    print(label_batch.numpy())\n",
        "\n",
        "#display the image\n",
        "class_names = dataset.class_names\n",
        "plt.figure(figsize = (10,10))\n",
        "for image_batch , label_batch in dataset.take(1):\n",
        "    for i in range(20):\n",
        "          ax = plt.subplot(4,5 ,i+1)\n",
        "          plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
        "          plt.title(class_names[label_batch[i]])\n",
        "          plt.axis(\"off\")\n",
        "\n",
        "\n",
        "#divide the data in the train_test and the validation_state\n",
        "len(dataset)\n",
        "train_size = 0.8\n",
        "len(dataset)*train_size\n",
        "train_ds = dataset.take(78)\n",
        "len(train_ds)\n",
        "test_size = 0.1\n",
        "len(dataset)*test_size\n",
        "test_ds = dataset.skip(78)\n",
        "len(test_ds)\n",
        "val_size = 0.1\n",
        "len(dataset)*val_size\n",
        "val_ds = test_ds.skip(10)\n",
        "len(val_ds)\n",
        "len(train_ds),len(test_ds) , len(val_ds)\n",
        "\n",
        "#after the divide the data in the training and the test_set\n",
        "\n",
        "from tensorflow.keras import layers, models,Sequential()\n",
        "\n",
        "#resizing and rescaling  the images\n",
        "resizing_and_rescaling=Sequential[(\n",
        "    layers.Resizing(image_size , image_size),\n",
        "    layers.Rescaling(1./255),\n",
        ")]\n",
        "\n",
        "#data agumentation\n",
        "data_agumentation  = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizental_and_verticle\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "])\n",
        "n_CHHANELS=3\n",
        "#lets  build the model for the image_data\n",
        "num_classes = 5\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Rescaling(1./255),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes)\n",
        "])"
      ],
      "metadata": {
        "id": "ZUUjwClDKT12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer= \"adam\", loss=\"SparseCategoricalCrossentropy\" , metrics = ['accuracy'])\n",
        "model.fit(train_ds ,batch_size =32 ,epochs =10 , validation_data = val_ds , verbose =1 )"
      ],
      "metadata": {
        "id": "dyL57UQWZ7dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video preprocessing"
      ],
      "metadata": {
        "id": "M0bBveJiaP-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "installation requeid for the loading the data"
      ],
      "metadata": {
        "id": "q6mBFeaVbdOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install remotezip tqdm opencv-python einops\n",
        "pip install -U tensorflow keras\n"
      ],
      "metadata": {
        "id": "HGR8G1tNagj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import einops\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "WpQ99zY5bosF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the data"
      ],
      "metadata": {
        "id": "3Rw46N4bc-_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\n",
        "download_dir = pathlib.Path('./UCF101_subset/')\n",
        "subset_paths = download_ufc_101_subset(URL,\n",
        "                        num_classes = 10,\n",
        "                        splits = {\"train\": 30, \"val\": 10, \"test\": 10},\n",
        "                        download_dir = download_dir)"
      ],
      "metadata": {
        "id": "0iU9QjGdcrYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"url name\"\n",
        "download_dir=pathlib.path(\"/\")\n",
        "subset_paths=dowm_ufc_101.subset(URL ,\n",
        "                                 num_classes = 10 ,\n",
        "                                 split={\"train\":30 , \"val\"=10 , \"test\"=10},\n",
        "                                 download_dir=download_dir)"
      ],
      "metadata": {
        "id": "chnbEityc-Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import einops\n",
        "\n",
        "# Define the dimensions of one frame in the set of frames created\n",
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "\n",
        "def conv2plus1d(x, filters, kernel_size, padding='same'):\n",
        "    \"\"\"Applies a Conv2Plus1D operation: spatial followed by temporal convolution.\"\"\"\n",
        "    # Spatial convolution\n",
        "    x = layers.Conv3D(filters=filters, kernel_size=(1, kernel_size[1], kernel_size[2]), padding=padding)(x)\n",
        "    # Temporal convolution\n",
        "    x = layers.Conv3D(filters=filters, kernel_size=(kernel_size[0], 1, 1), padding=padding)(x)\n",
        "    return x\n",
        "\n",
        "def residual_main(x, filters, kernel_size):\n",
        "    \"\"\"Creates the main path of the residual block with convolution, normalization, and ReLU.\"\"\"\n",
        "    y = conv2plus1d(x, filters, kernel_size)\n",
        "    y = layers.LayerNormalization()(y)\n",
        "    y = layers.ReLU()(y)\n",
        "    y = conv2plus1d(y, filters, kernel_size)\n",
        "    y = layers.LayerNormalization()(y)\n",
        "    return y\n",
        "\n",
        "def project(x, units):\n",
        "    \"\"\"Projects certain dimensions of the tensor as required.\"\"\"\n",
        "    x = layers.Dense(units)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    return x\n",
        "\n",
        "def add_residual_block(input_tensor, filters, kernel_size):\n",
        "    \"\"\"Adds a residual block, projecting dimensions if necessary.\"\"\"\n",
        "    residual = input_tensor\n",
        "    y = residual_main(input_tensor, filters, kernel_size)\n",
        "\n",
        "    if y.shape[-1] != residual.shape[-1]:\n",
        "        residual = project(residual, y.shape[-1])\n",
        "\n",
        "    return layers.add([residual, y])\n",
        "\n",
        "def resize_video(video, height, width):\n",
        "    \"\"\"Resizes video frames to the specified height and width.\"\"\"\n",
        "    old_shape = einops.parse_shape(video, 'b t h w c')\n",
        "    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
        "    images = layers.Resizing(height, width)(images)\n",
        "    resized_video = einops.rearrange(images, '(b t) h w c -> b t h w c', t=old_shape['t'])\n",
        "    return resized_video\n",
        "\n",
        "# Use the Keras functional API to build the residual network\n",
        "input_shape = (10, HEIGHT, WIDTH, 3)\n",
        "inputs = layers.Input(shape=input_shape)\n",
        "x = inputs\n",
        "\n",
        "# Initial layers\n",
        "x = conv2plus1d(x, filters=16, kernel_size=(3, 7, 7), padding='same')\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = tf.keras.layers.Lambda(lambda v: resize_video(v, HEIGHT // 2, WIDTH // 2))(x)\n",
        "\n",
        "# Block 1\n",
        "x = add_residual_block(x, 16, (3, 3, 3))\n",
        "x = tf.keras.layers.Lambda(lambda v: resize_video(v, HEIGHT // 4, WIDTH // 4))(x)\n",
        "\n",
        "# Block 2\n",
        "x = add_residual_block(x, 32, (3, 3, 3))\n",
        "x = tf.keras.layers.Lambda(lambda v: resize_video(v, HEIGHT // 8, WIDTH // 8))(x)\n",
        "\n",
        "# Block 3\n",
        "x = add_residual_block(x, 64, (3, 3, 3))\n",
        "x = tf.keras.layers.Lambda(lambda v: resize_video(v, HEIGHT // 16, WIDTH // 16))(x)\n",
        "\n",
        "# Block 4\n",
        "x = add_residual_block(x, 128, (3, 3, 3))\n",
        "\n",
        "# Final layers\n",
        "x = layers.GlobalAveragePooling3D()(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10)(x)\n",
        "\n",
        "# Create the model\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x=train_ds, epochs=50, validation_data=val_ds)\n"
      ],
      "metadata": {
        "id": "fY8NlJ52ddhz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}